# # æ ¹æ®å·²æœ‰çš„ä»£ç ï¼Œæ ¹æ®ç”¨æˆ·æä¾›çš„apiï¼Œç”Ÿæˆå¯¹åº”çš„ä»£ç 
# import os
#
# import docx
# from docx import Document
# from langchain_core.tools import tool
#
# from DB.chroma import ChromaDB
# from common.modeCommon import Model
# from config.config import Config
# from enums.model_enums import ModelType
# from knowledge import Knowledge
#
# import logging
#
# logger = logging.getLogger(__name__)
# def load_file(file_path: list[str], document_processor: Knowledge, chromadb: ChromaDB):
#     logger.info("å¼€å§‹åŠ è½½æ–‡ä»¶")
#     for file_path in file_path:
#         data = document_processor.load_files([file_path], "")
#         chromadb.add(data)
#         logger.info(f"åŠ è½½æ–‡ä»¶{file_path}æˆåŠŸ")
#
#
#
# @tool
# def read_docx(file_path):
#
#     """è¯»å–.docxæ–‡ä»¶å†…å®¹ï¼ˆä¸åŒ…æ‹¬é¡µçœ‰é¡µè„šï¼‰"""
#     if not os.path.exists(file_path):
#         print("æ–‡ä»¶ä¸å­˜åœ¨ï¼")
#         return ""
#     doc = Document(file_path)
#     full_text = []
#
#     # è¯»å–æ®µè½æ–‡æœ¬
#     for para in doc.paragraphs:
#         full_text.append(para.text)
#
#     # è¯»å–è¡¨æ ¼å†…å®¹
#     for table in doc.tables:
#         for row in table.rows:
#             for cell in row.cells:
#                 full_text.append(cell.text)
#
#     return "\n".join(full_text)
#
# def parse_api_info(model: Model, api_info: str):
#     """
#     è§£æapiä¿¡æ¯
#     :param model: æ¨¡å‹
#     :param api_info: apiä¿¡æ¯
#     :return: è§£æåçš„apiä¿¡æ¯ api_name, api_url, api_method, api_params, api_desc
#     """
#     prompt = f"""
#     ä½ æ˜¯ä¸€ä¸ªapiä¿¡æ¯è§£æå™¨ï¼Œä½ éœ€è¦è§£æä»¥ä¸‹apiä¿¡æ¯ï¼Œæå–api_name, api_url, api_method, api_params, api_desc
#     api_info: {api_info}
#     """
#     response = model.qwen_llm().invoke(prompt)
#     return response.content
#
# def parse_api_info_agent(model: Model, api_info: str):
#     return parse_api_info(model, api_info)
#
#
#
#
# if __name__ == '__main__':
#     # file_path = [
#     #     'D:\\project\\xdso-product\\codePecker-SAST-plugin\\src\\main\\java\\com\\shds\\product\\plugin\\CodePeckerSASTManage.java',
#     #     'D:\\project\\xdso-product\\codePecker-SAST-plugin\\src\\main\\java\\com\\shds\\product\\plugin\\CodePeckerSASTPlugin.java',
#     #     'D:\\project\\xdso-product\\codePecker-SAST-plugin\\src\\main\\java\\com\\shds\\product\\plugin\\CodePeckerSASTService.java',
#     #     'D:\\project\\xdso-product\\codePecker-SAST-plugin\\src\\main\\java\\com\\shds\\product\\plugin\\CodeVulnInfo.java',
#     #     'D:\\project\\xdso-product\\codePecker-SAST-plugin\\src\\main\\java\\com\\shds\\product\\plugin\\PagedResponse.java',
#     #     'D:\\project\\xdso-product\\codePecker-SAST-plugin\\src\\main\\java\\com\\shds\\product\\plugin\\CodeVulnInfo.java',
#     #     "D:\\project\\xdso-product\\xdso-product-api\\src\\main\\java\\com\\shds\\product\\domain\\sscs\\CommentReq.java",
#     #     "D:\\project\\xdso-product\\xdso-product-api\\src\\main\\java\\com\\shds\\product\\domain\\sscs\\CreateProjectReq.java",
#     #     "D:\\project\\xdso-product\\xdso-product-api\\src\\main\\java\\com\\shds\\product\\domain\\sscs\\DetectionInfo.java",
#     #     "D:\\project\\xdso-product\\xdso-product-api\\src\\main\\java\\com\\shds\\product\\domain\\sscs\\DetectionReq.java",
#     #     "D:\\project\\xdso-product\\xdso-product-api\\src\\main\\java\\com\\shds\\product\\domain\\sscs\\DetectionRes.java",
#     #     "D:\\project\\xdso-product\\xdso-product-api\\src\\main\\java\\com\\shds\\product\\domain\\sscs\\DetectionStatusRes.java",
#     #     "D:\\project\\xdso-product\\xdso-product-api\\src\\main\\java\\com\\shds\\product\\domain\\sscs\\DownloadReq.java",
#     #     "D:\\project\\xdso-product\\xdso-product-api\\src\\main\\java\\com\\shds\\product\\domain\\sscs\\DownloadReq.java"]
#     # document_processor = Knowledge()
#     config = Config('conf/config.yml')
#     # chromadb = ChromaDB(config, ModelType.QWEN)
#     # load_file(file_path, document_processor,chromadb)
#     model = Model(config)
#     api_info = read_docx(r"D:\api.docx")
#     if api_info:
#         print(f"åŸå§‹apiä¿¡æ¯ï¼š{api_info}")
#         print("="*30)
#         parse_api_info = parse_api_info_agent(model, api_info)
#
#         print(f"è§£æåçš„apiä¿¡æ¯ï¼š{parse_api_info}")
#     else:
#         print("æ–‡ä»¶ä¸å­˜åœ¨ï¼")
#
#     print(parse_api_info)

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import time

# è®¾ç½®é¡µé¢
st.set_page_config(
    page_title="ç³»ç»Ÿç›‘æ§è¯„åˆ†ä»ªè¡¨æ¿",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ ‡é¢˜å’Œè¯´æ˜
st.title("ğŸ“Š ç³»ç»Ÿç›‘æ§è¯„åˆ†ä»ªè¡¨æ¿")
st.markdown("""
è¯¥ä»ªè¡¨æ¿å±•ç¤ºç³»ç»Ÿå„é¡¹ç›‘æ§æŒ‡æ ‡çš„å®æ—¶çŠ¶æ€å’Œç»¼åˆè¯„åˆ†ï¼Œå¸®åŠ©æ‚¨äº†è§£ç³»ç»Ÿå¥åº·çŠ¶æ€ã€‚
""")


# æ¨¡æ‹Ÿç›‘æ§æ•°æ®è·å–å‡½æ•°
def fetch_monitor_data():
    """æ¨¡æ‹Ÿè·å–ç›‘æ§æ•°æ®"""
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„API
    current_time = datetime.now()

    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    data = {
        "timestamp": current_time.strftime("%Y-%m-%d %H:%M:%S"),
        "host": {
            "cpu_usage": np.random.uniform(10, 90),
            "memory_usage": np.random.uniform(20, 85),
            "disk_usage": np.random.uniform(30, 80)
        },
        "mysql": {
            "connections": np.random.randint(50, 200),
            "queries": np.random.randint(1000, 5000),
            "slow_queries": np.random.randint(0, 15)
        },
        "redis": {
            "hit_rate": np.random.uniform(85, 99.9),
            "connections": np.random.randint(10, 100),
            "memory_used": np.random.uniform(500, 2000)  # MB
        },
        "services": {
            "api_service": np.random.choice([0, 1, 2], p=[0.85, 0.1, 0.05]),
            "auth_service": np.random.choice([0, 1, 2], p=[0.9, 0.05, 0.05]),
            "database_service": np.random.choice([0, 1, 2], p=[0.95, 0.03, 0.02]),
            "cache_service": np.random.choice([0, 1, 2], p=[0.88, 0.08, 0.04])
        },
        "es": {
            "cpu_usage": np.random.uniform(15, 75),
            "memory_usage": np.random.uniform(25, 80),
            "disk_usage": np.random.uniform(40, 90)
        }
    }
    return data


# è®¡ç®—ç»¼åˆè¯„åˆ†
def calculate_score(data):
    """æ ¹æ®ç›‘æ§æ•°æ®è®¡ç®—ç»¼åˆè¯„åˆ†"""
    scores = {}

    # ä¸»æœºæŒ‡æ ‡è¯„åˆ† (æƒé‡: 30%)
    host_score = 100 - (
            max(0, data["host"]["cpu_usage"] - 70) * 0.5 +
            max(0, data["host"]["memory_usage"] - 75) * 0.3 +
            max(0, data["host"]["disk_usage"] - 80) * 0.2
    )
    scores["host"] = max(60, min(100, host_score))

    # MySQLæŒ‡æ ‡è¯„åˆ† (æƒé‡: 25%)
    mysql_score = 100 - (
            max(0, data["mysql"]["connections"] - 150) * 0.1 +
            data["mysql"]["slow_queries"] * 2
    )
    scores["mysql"] = max(60, min(100, mysql_score))

    # RedisæŒ‡æ ‡è¯„åˆ† (æƒé‡: 20%)
    redis_score = 100 - (
            (100 - data["redis"]["hit_rate"]) * 0.5 +
            max(0, data["redis"]["connections"] - 80) * 0.2 +
            max(0, data["redis"]["memory_used"] - 1500) * 0.01
    )
    scores["redis"] = max(60, min(100, redis_score))

    # æœåŠ¡çŠ¶æ€è¯„åˆ† (æƒé‡: 15%)
    service_status = data["services"]
    service_penalty = (
            (1 if service_status["api_service"] == 1 else 2 if service_status["api_service"] == 2 else 0) +
            (1 if service_status["auth_service"] == 1 else 2 if service_status["auth_service"] == 2 else 0) +
            (1 if service_status["database_service"] == 1 else 2 if service_status["database_service"] == 2 else 0) +
            (1 if service_status["cache_service"] == 1 else 2 if service_status["cache_service"] == 2 else 0)
    )
    service_score = 100 - service_penalty * 10
    scores["services"] = max(60, min(100, service_score))

    # ESæŒ‡æ ‡è¯„åˆ† (æƒé‡: 10%)
    es_score = 100 - (
            max(0, data["es"]["cpu_usage"] - 70) * 0.4 +
            max(0, data["es"]["memory_usage"] - 75) * 0.3 +
            max(0, data["es"]["disk_usage"] - 85) * 0.3
    )
    scores["es"] = max(60, min(100, es_score))

    # ç»¼åˆè¯„åˆ†
    weights = {
        "host": 0.3,
        "mysql": 0.25,
        "redis": 0.2,
        "services": 0.15,
        "es": 0.1
    }

    total_score = sum(scores[category] * weights[category] for category in scores)
    scores["total"] = total_score

    return scores


# çŠ¶æ€æŒ‡ç¤ºå™¨
def status_indicator(value, thresholds=[80, 90]):
    """ç”ŸæˆçŠ¶æ€æŒ‡ç¤ºå™¨"""
    if value < thresholds[0]:
        return st.success("æ­£å¸¸")
    elif value < thresholds[1]:
        return st.warning("è­¦å‘Š")
    else:
        return st.error("å¼‚å¸¸")


# ç”Ÿæˆå†å²æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰
@st.cache_data(ttl=300)
def generate_historical_data(hours=24):
    """ç”Ÿæˆå†å²ç›‘æ§æ•°æ®"""
    history = []
    current_time = datetime.now()

    for i in range(hours):
        timestamp = current_time - timedelta(hours=i)
        data = fetch_monitor_data()
        scores = calculate_score(data)

        history.append({
            "timestamp": timestamp,
            "scores": scores,
            "data": data
        })

    return history


# ä¾§è¾¹æ 
st.sidebar.header("é…ç½®é€‰é¡¹")
auto_refresh = st.sidebar.checkbox("è‡ªåŠ¨åˆ·æ–°", value=True)
refresh_interval = st.sidebar.slider("åˆ·æ–°é—´éš”(ç§’)", min_value=5, max_value=60, value=15)

if auto_refresh:
    st.sidebar.write(f"ä¸‹æ¬¡åˆ·æ–°: {refresh_interval}ç§’")
    time.sleep(refresh_interval)
    st.experimental_rerun()

# è·å–æ•°æ®
data = fetch_monitor_data()
scores = calculate_score(data)
history = generate_historical_data()

# æ€»ä½“è¯„åˆ†å¡ç‰‡
col1, col2, col3, col4 = st.columns(4)

with col1:
    st.metric("ç»¼åˆè¯„åˆ†", f"{scores['total']:.1f}", delta="-2.5" if scores['total'] < 90 else "+1.2")

with col2:
    st.metric("ä¸»æœºè¯„åˆ†", f"{scores['host']:.1f}")

with col3:
    st.metric("MySQLè¯„åˆ†", f"{scores['mysql']:.1f}")

with col4:
    st.metric("Redisè¯„åˆ†", f"{scores['redis']:.1f}")

# è¯„åˆ†è¶‹åŠ¿å›¾
st.subheader("è¯„åˆ†è¶‹åŠ¿")
history_df = pd.DataFrame([{
    "æ—¶é—´": h["timestamp"],
    "ç»¼åˆè¯„åˆ†": h["scores"]["total"],
    "ä¸»æœºè¯„åˆ†": h["scores"]["host"],
    "MySQLè¯„åˆ†": h["scores"]["mysql"],
    "Redisè¯„åˆ†": h["scores"]["redis"],
    "æœåŠ¡è¯„åˆ†": h["scores"]["services"],
    "ESè¯„åˆ†": h["scores"]["es"]
} for h in history])

fig = px.line(history_df, x="æ—¶é—´", y=["ç»¼åˆè¯„åˆ†", "ä¸»æœºè¯„åˆ†", "MySQLè¯„åˆ†", "Redisè¯„åˆ†", "æœåŠ¡è¯„åˆ†", "ESè¯„åˆ†"],
              title="è¿‡å»24å°æ—¶è¯„åˆ†è¶‹åŠ¿")
st.plotly_chart(fig, use_container_width=True)

# è¯¦ç»†ä¿¡æ¯é€‰é¡¹å¡
tab1, tab2, tab3, tab4, tab5 = st.tabs(["ä¸»æœºæŒ‡æ ‡", "MySQLæŒ‡æ ‡", "RedisæŒ‡æ ‡", "æœåŠ¡çŠ¶æ€", "ESæŒ‡æ ‡"])

with tab1:
    st.subheader("ä¸»æœºç›‘æ§æŒ‡æ ‡")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("CPUä½¿ç”¨ç‡", f"{data['host']['cpu_usage']:.1f}%")
        status_indicator(data['host']['cpu_usage'])

    with col2:
        st.metric("å†…å­˜ä½¿ç”¨ç‡", f"{data['host']['memory_usage']:.1f}%")
        status_indicator(data['host']['memory_usage'])

    with col3:
        st.metric("ç£ç›˜ä½¿ç”¨ç‡", f"{data['host']['disk_usage']:.1f}%")
        status_indicator(data['host']['disk_usage'])

    # ä¸»æœºæŒ‡æ ‡è¶‹åŠ¿
    host_history = pd.DataFrame([{
        "æ—¶é—´": h["timestamp"],
        "CPUä½¿ç”¨ç‡": h["data"]["host"]["cpu_usage"],
        "å†…å­˜ä½¿ç”¨ç‡": h["data"]["host"]["memory_usage"],
        "ç£ç›˜ä½¿ç”¨ç‡": h["data"]["host"]["disk_usage"]
    } for h in history])

    fig = px.line(host_history, x="æ—¶é—´", y=["CPUä½¿ç”¨ç‡", "å†…å­˜ä½¿ç”¨ç‡", "ç£ç›˜ä½¿ç”¨ç‡"],
                  title="ä¸»æœºæŒ‡æ ‡è¶‹åŠ¿")
    st.plotly_chart(fig, use_container_width=True)

with tab2:
    st.subheader("MySQLç›‘æ§æŒ‡æ ‡")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("è¿æ¥æ•°", data['mysql']['connections'])
        status_indicator(data['mysql']['connections'], [150, 180])

    with col2:
        st.metric("æŸ¥è¯¢æ•°", data['mysql']['queries'])

    with col3:
        st.metric("æ…¢æŸ¥è¯¢æ•°", data['mysql']['slow_queries'])
        status_indicator(data['mysql']['slow_queries'], [5, 10])

    # MySQLæŒ‡æ ‡è¶‹åŠ¿
    mysql_history = pd.DataFrame([{
        "æ—¶é—´": h["timestamp"],
        "è¿æ¥æ•°": h["data"]["mysql"]["connections"],
        "æŸ¥è¯¢æ•°": h["data"]["mysql"]["queries"],
        "æ…¢æŸ¥è¯¢æ•°": h["data"]["mysql"]["slow_queries"]
    } for h in history])

    fig = px.line(mysql_history, x="æ—¶é—´", y=["è¿æ¥æ•°", "æŸ¥è¯¢æ•°", "æ…¢æŸ¥è¯¢æ•°"],
                  title="MySQLæŒ‡æ ‡è¶‹åŠ¿")
    st.plotly_chart(fig, use_container_width=True)

with tab3:
    st.subheader("Redisç›‘æ§æŒ‡æ ‡")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("å‘½ä¸­ç‡", f"{data['redis']['hit_rate']:.1f}%")
        status_indicator(data['redis']['hit_rate'], [90, 95])

    with col2:
        st.metric("è¿æ¥æ•°", data['redis']['connections'])
        status_indicator(data['redis']['connections'], [70, 85])

    with col3:
        st.metric("å†…å­˜ä½¿ç”¨(MB)", f"{data['redis']['memory_used']:.1f}")
        status_indicator(data['redis']['memory_used'], [1500, 1800])

    # RedisæŒ‡æ ‡è¶‹åŠ¿
    redis_history = pd.DataFrame([{
        "æ—¶é—´": h["timestamp"],
        "å‘½ä¸­ç‡": h["data"]["redis"]["hit_rate"],
        "è¿æ¥æ•°": h["data"]["redis"]["connections"],
        "å†…å­˜ä½¿ç”¨": h["data"]["redis"]["memory_used"]
    } for h in history])

    fig = px.line(redis_history, x="æ—¶é—´", y=["å‘½ä¸­ç‡", "è¿æ¥æ•°", "å†…å­˜ä½¿ç”¨"],
                  title="RedisæŒ‡æ ‡è¶‹åŠ¿")
    st.plotly_chart(fig, use_container_width=True)

with tab4:
    st.subheader("æœåŠ¡çŠ¶æ€ç›‘æ§")

    service_status = data['services']
    status_map = {0: "æ­£å¸¸", 1: "å¼‚å¸¸", 2: "è­¦å‘Š"}
    status_color = {0: "green", 1: "red", 2: "orange"}

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.markdown(f"**APIæœåŠ¡**: <span style='color:{status_color[service_status['api_service']]}'>"
                    f"{status_map[service_status['api_service']]}</span>",
                    unsafe_allow_html=True)

    with col2:
        st.markdown(f"**è®¤è¯æœåŠ¡**: <span style='color:{status_color[service_status['auth_service']]}'>"
                    f"{status_map[service_status['auth_service']]}</span>",
                    unsafe_allow_html=True)

    with col3:
        st.markdown(f"**æ•°æ®åº“æœåŠ¡**: <span style='color:{status_color[service_status['database_service']]}'>"
                    f"{status_map[service_status['database_service']]}</span>",
                    unsafe_allow_html=True)

    with col4:
        st.markdown(f"**ç¼“å­˜æœåŠ¡**: <span style='color:{status_color[service_status['cache_service']]}'>"
                    f"{status_map[service_status['cache_service']]}</span>",
                    unsafe_allow_html=True)

    # æœåŠ¡çŠ¶æ€å†å²
    service_history = pd.DataFrame([{
        "æ—¶é—´": h["timestamp"],
        "APIæœåŠ¡": h["data"]["services"]["api_service"],
        "è®¤è¯æœåŠ¡": h["data"]["services"]["auth_service"],
        "æ•°æ®åº“æœåŠ¡": h["data"]["services"]["database_service"],
        "ç¼“å­˜æœåŠ¡": h["data"]["services"]["cache_service"]
    } for h in history])

    fig = px.line(service_history, x="æ—¶é—´", y=["APIæœåŠ¡", "è®¤è¯æœåŠ¡", "æ•°æ®åº“æœåŠ¡", "ç¼“å­˜æœåŠ¡"],
                  title="æœåŠ¡çŠ¶æ€å†å²ï¼ˆ0=æ­£å¸¸, 1=å¼‚å¸¸, 2=è­¦å‘Šï¼‰")
    st.plotly_chart(fig, use_container_width=True)

with tab5:
    st.subheader("Elasticsearchç›‘æ§æŒ‡æ ‡")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("CPUä½¿ç”¨ç‡", f"{data['es']['cpu_usage']:.1f}%")
        status_indicator(data['es']['cpu_usage'])

    with col2:
        st.metric("å†…å­˜ä½¿ç”¨ç‡", f"{data['es']['memory_usage']:.1f}%")
        status_indicator(data['es']['memory_usage'])

    with col3:
        st.metric("ç£ç›˜ä½¿ç”¨ç‡", f"{data['es']['disk_usage']:.1f}%")
        status_indicator(data['es']['disk_usage'])

    # ESæŒ‡æ ‡è¶‹åŠ¿
    es_history = pd.DataFrame([{
        "æ—¶é—´": h["timestamp"],
        "CPUä½¿ç”¨ç‡": h["data"]["es"]["cpu_usage"],
        "å†…å­˜ä½¿ç”¨ç‡": h["data"]["es"]["memory_usage"],
        "ç£ç›˜ä½¿ç”¨ç‡": h["data"]["es"]["disk_usage"]
    } for h in history])

    fig = px.line(es_history, x="æ—¶é—´", y=["CPUä½¿ç”¨ç‡", "å†…å­˜ä½¿ç”¨ç‡", "ç£ç›˜ä½¿ç”¨ç‡"],
                  title="ESæŒ‡æ ‡è¶‹åŠ¿")
    st.plotly_chart(fig, use_container_width=True)

# åº•éƒ¨ä¿¡æ¯
st.markdown("---")
st.markdown(f"æœ€åæ›´æ–°: {data['timestamp']} | æ•°æ®æ¥æº: ç›‘æ§ç³»ç»ŸAPI")